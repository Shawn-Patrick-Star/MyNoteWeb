# README——写在最前面

最新更新时间：2022.11.30

更新内容：

- 2~5章进行内部内容的系统化调整，更方便记忆
- 假设检验没有写完，但是鉴于和置信区间相似，其实不写也可以
- 置信区间这部分没有写完，请看书
- *使用伽马函数简化运算* 很重要！！！一定要掌握
- 注意：相关 & 独立 & 不相容（互斥）& 对立 的区别



# 一.随机事件与概率

## 基本概念

|                  |                              |
| ---------------- | ---------------------------- |
| 样本点$\omega$   | 随机试验的不可再分的可能结果 |
| 样本空间$\Omega$ | 样本点全体组成的集合         |

样本点数可以是有限的也可以是无限的

|                  |                                                             |
| ---------------- | ----------------------------------------------------------- |
| 随机事件         | 可能发生也可能不发生的事情                                  |
| 基本事件         | 由一个样本点组成                                            |
| 必然事件$\Omega$ | 一定发生，=样本空间<br />$P(\Omega)=1,但是PA=1\ne A=\Omega$ |
| 不可能事件$\phi$ | 不可能发生<br />$P(\phi)=0,但是PA=0\ne A=\phi$              |

|               |                                                              |
| ------------- | ------------------------------------------------------------ |
| 独立试验      | 试验对应的事件相互独立                                       |
| 独立重复试验  | 独立——试验对应的事件相互独立，重复——每个事件在各次试验中出现的概率不变 |
| 伯努利试验    | 只计“成功”和“失败”两种对立结果的试验                         |
| n重伯努利试验 | 伯努利试验独立地重复进行n次                                  |

## 事件

> 用  集合来定义事件

|                |                 |                                                          |
| -------------- | --------------- | -------------------------------------------------------- |
| 包含           | $A \subset B$   | A发生必然导致B发生（$P(B|A)=1$），B包含A                 |
| 相等           | $A=B$           | 同一事件表面上看起来不同的两个说法而已，包含的样本点相同 |
| 等价           | $P(A)=P(B)$     | 两个事件的概率相等                                       |
| 交             | $A \cap B = AB$ | A，B同时发生                                             |
| 并             | $A\cup B$       | A，B至少有一个发生                                       |
| 互不相容(互斥) | $AB= \phi$      |                                                          |
| 对立(逆)       | $\overline{A}$  | A不发生                                                  |
| 差             | $A-B$           | A发生且B不发生                                           |
| 完备事件组     |                 |                                                          |

互斥可以对于多个事件，对立只针对2个事件

### 集合运算公式

$\overline{C}\sub C\to C=\Omega$

$A-B=A-(AB)=A\overline{B}$（通过画图来理解）

$A=(AB)\cup(A\overline{B}),B=(AB)\cup(\overline{A}B)$（通过画图来理解）

## 概率

![image-20230919105855346](_media/pic//image-20230919105855346.png)



### 概率运算公式

|              |                                                              |
| ------------ | :----------------------------------------------------------: |
| 有界性       |                       $0\le P(A)\le1$                        |
| 包含         |            $A\sub B$，则$P(A)\le P(B),P(A)=P(AB)$            |
| 放缩         |            $P(ABC)\le P(AB) \le P(A) \le P(A+B)$             |
| 取逆         |                   $P(\overline{A})=1-P(A)$                   |
| 德摩根定律   |      $\overline{A\cup B}=\overline{A}\cap\overline{B}$       |
| 加法公式     |                 $P(A\cup B)=P(A)+P(B)-P(AB)$                 |
|              |  $P(A\cup B\cup C)=P(A)+P(B)+P(C)-P(AB)-P(AC)-P(BC)+P(ABC)$  |
| 减法公式     |             $P(A-B)=P(A)-P(AB)=P(A\overline{B})$             |
| 条件概率公式 |                 $P(A|B)=\dfrac{P(AB)}{P(B)}$                 |
| 乘法公式     |   $P(AB)=P(A)P(B|A)$<br />$P(AB)=P(A)P(B)$（若$A,B$独立）    |
| 全概率公式   |           $P(B)=\sum P(A_iB)=\sum P(A_i)P(B|A_i)$            |
| 贝叶斯公式   | $P(A_i|B)=\dfrac{P(A_i)P(B|A_i)}{P(B)}=\dfrac{P(A_i)P(B|A_i)}{\sum P(A_i)P(B|A_i)}$ |
| 最值         | <img src="_media/pic//image-20231212211601446.png" alt="image-20231212211601446" style="zoom: 80%;" /> |

 

### 全概率&贝叶斯

[概率学派和贝叶斯学派的区别 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158933171)

![image-20230920111752676](_media/pic//image-20230920111752676.png)

用这种树的形式去理解全概率公式和贝叶斯公式

全概率公式：它将对一复杂事件B的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题

| 贝叶斯公式 |                                                        |
| ---------- | ------------------------------------------------------ |
| 本质       | 衡量一个变量（结果）多大程度的依赖于另一个变量（原因） |
| 重要思想   | 不断通过新的信息，调整原有的思维方式                   |

理解贝叶斯公式：

假设选择题有三个选项A,B,C.  A,B选项有关联. 现在有参数θ=A,B,C表示真正正确的选项，以及随机变量X=1.2.3表示考生做出的选择A,B,C。
在你的信念里，θ是有一个概率分布的，假设它集中在θ=A附近(B其次，C很少很少).
情况①.现在你考完试，打探到一大堆同学选的是B. 这时你会觉得发慌，觉得自己有可能是错的。此时其实你对于θ分布的估计重新发生了改变：变成了θ在B附近集中分布，A次之，C很少很少
情况②. 当你考完，有两三个同学说这道题选C.你会暗自鄙视他们：怎么可能选C呢，C错得那么明显(也就是在你先验的认知里θ在C处分布得很少很少)。只有在相当大一部分包括绝大多数好成绩的人都选了C时，你的对于θ的估计，才会偏移集中到θ上(此时相当于大样本下贝叶斯回归MLE)
总结，贝叶斯估计本质上就是，我们最开始对世界(参数θ)有一个先验的认知，而我们会根据后天的观测结果来调整我们的认知判断。其实，这是符合辩证法里实践作用于意识的基本范式。
即：实践以一种什么样的形式影响于我们对世界的认知？答：贝叶斯估计。(p.s.意识对于实践的指导作用，工程学等很明显就不再赘述)

<img src="_media/pic//image-20231009164036623.png" alt="image-20231009164036623" style="zoom:50%;" />

<img src="_media/pic//image-20231009164128499.png" alt="image-20231009164128499" style="zoom: 67%;" />



### 古典概型

| 特征        |                              |
| ----------- | ---------------------------- |
| 1. 有限性   | 有限个样本点                 |
| 2. 等可能性 | 每个样本点发生的可能性都一样 |

![image-20230919110139257](_media/pic//image-20230919110139257.png)

![image-20230919181539842](_media/pic//image-20230919181539842.png)

注意这里的计数原理的应用

排列&组合要满足**定义**，他们都是从n个元素中取出m个元素

下面以一个例子来说明这个问题，

5人共钓到3条鱼，每条鱼被各人钓到的可能性相同，求：

（1）3条鱼由不同的人钓到的概率

（2）有1人钓到2条鱼的概率

（3）3条鱼由同一人钓到的概率

所有事件发生的个数是$5^3$，因为5个人里面我们不知道有几个人能钓到鱼，但是我们知道这3条鱼一定都被钓到了，所以站在鱼的角度，第一条鱼可以分给5个人，第二条鱼可以分给5个人，第三条鱼可以分给5个人，即$5*5*5=5^3$，这里鱼和人就不是一一对应的关系，因为一个人可能钓多条鱼或0条鱼。

如果不满足定义，需要先分步成符合定义的情形，再使用排列组合和乘法原理，比如（2），一人钓到两条鱼，先从5个人中选出1个人，再从3条鱼中选出2条鱼 

另外还可以通过判断前后两次实验是不是**随机实验**，比如第一条鱼被钓到之后是否影响第二条鱼被钓的概率，如果是就可能可以用排列组合解决，如果否，一定是乘法原理解决

还有一点，**乘法原理默认已经排序**，有时需要除以$P_n^n$来消除排序，比如平均分组问题，可以理解一下这句话：*当组合的对象等价时并且用了分步组合的方法时需要除序*

#### 随机分配

![image-20230919181323661](_media/pic//image-20230919181323661.png)

#### 随机抽样

![image-20230919184556577](_media/pic//image-20230919184556577.png)

## 事件独立性

> **注意：**事件独立和互斥（互不相容）没有关系，两个事件可以独立且互斥，也可以不独立且不互斥

### 两个事件

A，B两个事件，满足$P(AB)=P(A)P(B)$，则称A，B相互独立

| 性质                                                         |
| ------------------------------------------------------------ |
| 若$P(B)>0$，A，B相互独立的充要条件是$P(A|B)=P(A)$            |
| 若A，B相互独立，则$\overline{A},B;A,\overline{B};\overline{A},\overline{B}$ 均相互独立 |
| $\Omega$和$\phi$与任何事件都相互独立                         |
| **$P=0/1$的事件与任何事件都相互独立**                        |

### 多个事件

#### 两两独立

$P(AB)=P(A)(B)~~~~P(AC)=P(A)(C)~~~~P(BC)=P(B)(C)$



#### 相互独立

$P(AB)=P(A)P(B)~~~~P(AC)=P(A)P(C)~~~~P(BC)=P(B)P(C)$

$P(ABC)=P(A)P(B)P(C)$



# 二.一维变量

>虽然随机变量X可以取遍整个实数R，但我们只关心**有效取值范围**!!!
>
>即**概率强度**>0的范围，因为=0 求分布函数时 求和 / 积分 没有意义
>
>对于离散型，X的有效取值范围是几个点
>
>对于连续型，X的有效取值范围是几个区间

## 随机变量

“随机变量不同于代数中的变量，因为它具有一组完整的值，并且可以随机获取任何值。代数中使用的变量一次不能具有多个值。

如果随机变量X = {0,1,2,3} 那么X可以是随机的0、1、2或3，其中每个都有不同的概率。”

|                       |                                                  |                                                              |
| --------------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| 随机变量$X=X(\omega)$ | 定义在样本空间$\Omega$，取值于实数轴上的**函数** | 量化了随机事件，是事件→数字的映射                            |
| 分布函数              | $F(x)=P\{X\le x\},x\in(-\infin, +\infin)$        | 随机变量是以样本点为自变量，难以运用微积分工具，所以发明了分布函数，以实数为自变量 |



|          | 离散                   | 连续                                              |
| -------- | ---------------------- | ------------------------------------------------- |
|          | $X\sim p_i$            | $X\sim f(x)$                                      |
|          | 分布律                 | 概率密度函数                                      |
| 分布函数 | $F(x)=\sum P_i$        | $\displaystyle{F(x)=\int_{-\infin}^xf(t)dt}$      |
|          | 阶梯形                 | **连续函数，满足左连续**                          |
| 归一性   | $\sum P_i=1$           | $\displaystyle{\int_{-\infin}^{+\infin}f(t)dt}=1$ |
| 非负性   | $P_i\ge0$              | $f(x)\ge0$                                        |
| 某点概率 | $P\{X=a\}=F(a)-F(a-0)$ | $P\{X=a\}=0$                                      |
| 区间概率 | 与区间是否有 = 有关    | 积分                                              |



## 判分布/求参数

| 充要条件                      | 判断/求参数                                                  |
| ----------------------------- | ------------------------------------------------------------ |
| $F(x)$是分布函数              | 1.单调不减<br />2.右连续（$F(a+0)=F(a)$）<br />3.$F(-\infin)=0,F(+\infin)=1$ |
| 注：$F(x)$是连续型分布函数    | 第2条改成：连续（$F(a+0)=F(a-0)=F(a)$）                      |
| $\{p_i\}$是概率分布（分布律） | $p_i\ge0,\sum p_i=1$                                         |
| $f(x)$是概率密度              | $f(x)\ge0,\displaystyle{\int_{-\infin}^{+\infin}f(x)dx}=1$   |

注意：

已知f(x)求参数一般有两种情况：

1.（一般情况）直接积分

2.（正态分布）$f(x)=e^{x^2}$，考虑凑正态分布表达式（因为你不会求积分捏~）

## 求分布

### 分布函数

|                                                |                                                              |
| ---------------------------------------------- | ------------------------------------------------------------ |
| 离散型$X\sim p_i$                              | $\displaystyle{F(x)=\sum_{x_i\le x} P_i}$                    |
| 连续型$X\sim f(x)$                             | $\displaystyle{F(x)=\int_{-\infin}^xf(t)dt}$<br />注意：我们一般会把F(x)分段成$a\le x< b$的形式，这是为了保证右连续 |
| 混合型（不能用上面两种就用这种，适用范围最广） | $F(x)=P\{X\le x\}$<br />一般$\{X\le x\}$是复杂事件，用全概率公式 |

如果F(x)既不是阶梯函数，又不是连续函数，那么X就是混合型随机变量

### 概率密度/分布列

| 概率密度——分布函数 |                                                              |
| :----------------- | ------------------------------------------------------------ |
| $F(x)\to f(x)$     | $f(x)=F'(x)$<br />F(x)求f(x)不具有唯一性，所以不需要考虑间断点的导数情况，直接划分到 *“其他 ”* 情况 |
| $F(x)\to p_i$      | $P\{X= a\}=F(a)-F(a-0)$                                      |



## 求概率

| 应用——求概率 |                                                              |
| ------------ | ------------------------------------------------------------ |
| $X\sim F(x)$ | $P\{X\le a\}=F(a)$<br />$P\{X< a\}=F(a-0)$<br />$P\{X= a\}=F(a)-F(a-0)$ |
| $X\sim p_i$  | $P\{X\in I\}=\sum p_i$                                       |
| $X\sim f(x)$ | $\displaystyle{P\{X\in I\}=\int_I f(x)dx}$                   |



# 常见分布

| 八大分布   |                          |
| ---------- | ------------------------ |
| 两点分布   | $X\sim B(1,p)$           |
| 二项分布   | $X\sim B(n,p)$           |
| 泊松分布   | $X\sim P(\lambda)$       |
| 超几何分布 |                          |
| 几何分布   |                          |
| 均匀分布   | $X\sim U(a,b)$           |
| 指数分布   | $X\sim Exp(\lambda)$     |
| 正态分布   | $X\sim N(\mu, \sigma^2)$ |

## 离散型

### 两点分布(0-1分布)

$X\sim B(1,p)$ 

一次伯努利实验
$$
P\{X=k\}=p^k(1-p)^{1-k}~~~~~~~~k=0,1
$$

### 二项分布

$X\sim B(n,p)$

n重伯努利实验
$$
P\{X=k\}=C_n^k~p^k(1-p)^{n-k}~~~~~~~~k=0,1,...,n
$$

$C_n^k=\dfrac{n\times(n-1)\times...\times(n+1-k)}{k!}$

### 泊松分布

$X\sim P(\lambda)$

泊松分布来自于二项分布，是二项分布的极限情况——当$p\to0,n\to\infin$时，令$\lambda=np$

此时可以用泊松分布来近似二项分布（因为好算）

|          | 泊松分布                                                     | 二项分布                                                     |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 参数     | $\lambda$                                                    | $n,p$                                                        |
| 应用场景 | 用于描述在固定**时间段**或**空间区域**内某事件发生的次数的概率分布 | 此时的实验次数被无限细分，无法确定n的值，此时就该泊松分布大展身手了 |

$$
P\{X=k\}=\dfrac{\lambda^ke^{-\lambda}}{k!}~~~~~~~~k=0,1,...;
$$

### 超几何分布

N个产品，M个不合格品，不放回抽取n个（n<=N），其中不合格品的个数X服从超几何分布

它和二项分布很像，本质区别在于：二项分布进行n次**伯努利实验**，超几何分布进行n次**非独立实验**

| 我们看一个例子：                                             |            |
| ------------------------------------------------------------ | ---------- |
| 有13个小球，4个蓝色，9个红色，随机抽5个小球，每次抽1个，**且放回**，求抽到3个蓝色球的概率 | 二项分布   |
| 有13个小球，4个蓝色，9个红色，随机抽5个小球，每次抽1个，**且不放回**，求抽到3个蓝色球的概率 | 超几何分布 |

$$
P\{X=k\}=\dfrac{C_M^kC_{N-M}^{n-k}}{C_N^n}~~~~~~~~k=0,1,...,min\{n,M\}
$$

当n<<N时可以用二项分布来近似

### 几何分布

**也叫离散型“等待”分布**

一直做伯努利实验直到第一次成功即停止，即：前n-1次均失败，第n次成功

是二项分布中的一种
$$
P\{X=k\}=(1-p)^{k-1}p~~~~~~~~k=0,1,...
$$
无记忆性

$P\{X>m+n|X>m\}=P\{X>n\}$

## 连续型

### 均匀分布

其背景是一维几何概型

记作$X\sim U(a,b)$

|      | 概率密度                                                     | 分布函数                                                     |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 函数 | ![image-20231010193857298](_media/pic//image-20231010193857298.png) | ![image-20231010193906809](_media/pic//image-20231010193906809.png) |
| 图像 | ![image-20231010193926177](_media/pic//image-20231010193926177.png) | ![image-20231010193936378](_media/pic//image-20231010193936378.png) |

### 指数分布

$X\sim Exp(\lambda)$

**也叫连续型等待分布**

$\lambda$——失效率

|      | 概率密度                                                     | 分布函数                                                     |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 函数 | ![image-20231010195243831](_media/pic//image-20231010195243831.png) | ![image-20231010195256334](_media/pic//image-20231010195256334.png) |
| 图像 | ![image-20231010195304028](_media/pic//image-20231010195304028.png) | ![image-20231010195311282](_media/pic//image-20231010195311282.png) |

无记忆性

$P\{X=m+n|X>m\}=P\{X=n\}$

### 正态分布

记为$X\sim N(\mu, \sigma^2)$

|          | 正态分布                                                     | 标准正态分布                                                 |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 概率密度 | $f(x)=\dfrac{1}{\sqrt{2\pi}~\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}$ | $\phi(x)=$                                                   |
| 分布函数 | $\displaystyle{F(x)=\dfrac{1}{\sqrt{2\pi}~\sigma}\int_{-\infin}^x e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}}$ | ![image-20231011134834773](_media/pic//image-20231011134834773.png) |
| 图像     | ![image-20231011134751850](_media/pic//image-20231011134751850.png) | ![image-20231011134812370](_media/pic//image-20231011134812370.png) |

![image-20231011140731374](_media/pic//image-20231011140731374.png)

![image-20231011140713557](_media/pic//image-20231011140713557.png)

# 三.一维函数

> 已知X的分布，求Y=g(x)的分布

## 离散$\to$离散

非常简单，直接代入X求Y，合并概率（如果需要的话），即可得到分布列

<img src="_media/pic//image-20231016110046005.png" alt="image-20231016110046005" style="zoom:67%;" />

1.写出Y可能的取值

2.依次求解P{Y=k}

3.写成分布列（表格）

## 连续$\to$连续

|                |            |
| -------------- | :--------- |
| $y=g(x)$单调   | 公式法     |
| $y=g(x)$不单调 | 分布函数法 |

### 分布函数法

> 由分布函数求概率密度

核心：通过$F_X(x)$将$F_Y(y)$求解出来
$$
F_Y(y)=P\{Y\le y\}=P\{g(X)\le y\}=\int_{g(x)\le y}f_X(x)dx
$$

| 关键在于如何处理$P\{g(X)\le y\}$    |                                                              |
| ----------------------------------- | ------------------------------------------------------------ |
| 情况1（$g(X)$单调）：$Y=2X+1\le y$  | $X\le \dfrac{y-1}{2}$                                        |
| 情况2（$g(X)$不单调）：$Y=X^2\le y$ | 找到曲线在直线下方的**x的取值范围**：$X\in I$                |
| 情况3（$g(X)$分段）                 | 画出分段曲线，找到曲线在直线下方的**x的取值范围**（其实没区别） |

题目让求$f_Y(y)$，可以只写出$F_Y(y)=H\left(F_X(x)\right)$，等号左右同时求导得$f_Y(y)$

$\left(\displaystyle{\int_{g(x)}^{\phi(x)}}f(t)dt\right)'=f(\phi(x))\phi'(x)-f(g(x))g'(x)$

### 公式法

> 本质就是分布函数法，不过是进行简化，可以直接写出来不用推导了

前提：$y=g(x)$在$(a,b)$上是**严格单调**可导函数

|                                                            |
| :--------------------------------------------------------- |
| 1.求反函数$x=h(y)$                                         |
| 2.计算$Y$的有效区间$x\in I\to h(y)\in I$，也可以通过画图看 |
| 3.计算得到$y\in D时,f_Y(y)=f_X(h(y))\times|h'(y)|$         |

# 四.二维变量  

$$
联合分布函数\\
F(x,y)=P\{X\le x, Y\le y\}
$$

## 判分布/求参数

| 充要条件             | 判断/求参数                                                  |
| -------------------- | ------------------------------------------------------------ |
| $F(x)$是联合分布函数 | 1.单调不减<br />2.右连续（$F(a+0)=F(a)$）<br />3.$F(-\infin,y)=F(x,-\infin)=0,F(+\infin,+\infin)=1$ |
| $\{p_i\}$是分布律    | $p_{ij}\ge0,\sum\sum p_{ij}=1$                               |
| $f(x)$是概率密度     | $f(x,y)\ge0,\displaystyle{\int_{-\infin}^{+\infin}\int_{-\infin}^{+\infin}f(x,y)dxdy}=1$ |

## 求分布

### 联合分布

|             |                                                              |
| ----------- | ------------------------------------------------------------ |
| X，Y均离散  | 分别求出$P\{X=x_i,Y=y_j\}=p_{ij}$                            |
| X，Y均连续  | 将平面分成不同区域，分区域求<br />![image-20231022120250660](_media/pic//image-20231022120250660.png) |
| X，Y混合    | 若X，Y独立：$F(x,y)=F_X(x)F_Y(y)$<br />一般情况：$F(x,y)=P\{X\le x, Y \le y\}$，用全概率公式 |
| 用条件&边缘 | **连续型：**<br />联合 = 边缘 * 条件<br /><img src="_media/pic//image-20231207142744931.png" alt="image-20231207142744931" style="zoom:67%;" /><br /><br />**离散型：**（已知两变量边缘分布 + 一个条件就可以解出联合分布)<br />画表格 |

### 边缘分布

> 直观上是求一条竖（横）线上的概率是多少

|        |                                           |
| ------ | :---------------------------------------- |
|        | $F_X(x)=F(x,+\infin)$                     |
| 离散型 | $p_{i~\dot{}}=\sum_{j}p_{ij}$             |
| 连续型 | $f_X(x)=\int_{-\infin}^{+\infin}f(x,y)dy$ |

### 条件分布

| 注意保证 分母 > 0 !!!，分母=0时条件分布=0 |                                                              |
| ----------------------------------------- | ------------------------------------------------------------ |
| 离散型                                    | <img src="_media/pic//image-20231027173123775.png" alt="image-20231027173123775" style="zoom: 80%;" /> |
| 连续型                                    | $f_{Y|X}(y|x)=\dfrac{f(x,y)}{f_X(x)}$<br /><br />$f_{Y|X}(y|x=x_0)=\dfrac{f(x_0,y)}{f_X(x_0)}$<br /><br />注意：<br />x是参数，根据$f_X(x)$的不同表达式分x的区间（为了得到确定的$f_X(x)$）<br />y是变量，根据$f(x,y)$的不同表达式分y的区间（只求出a<y<x，不要a<y<x<b这种） |

## 求概率
$P\{a< X\le b, c< Y\le d\}=F(b,d)-F(a,d)-F(b,c)+F(a,c)\\P\{X>a,Y>b\}=1-P\{X\le a\}-P\{Y\le b\}+P\{X\le a,Y\le b\}\\P\{a< X<b,Y\le c\}=F(b-0,c)-F(a,c)$
| 应用——求概率       |                                                              |
| ------------------ | :----------------------------------------------------------: |
| $(X,Y)\sim F(x,y)$ | $P\{a< X\le b, c< Y\le d\}=F(b,d)-F(a,d)-F(b,c)+F(a,c)\\P\{X>a,Y>b\}=1-P\{X\le a\}-P\{Y\le b\}+P\{X\le a,Y\le b\}\\P\{a< X<b,Y\le c\}=F(b-0,c)-F(a,c)$ |
| $(X,Y)\sim p_{ij}$ |                    $P\{X\in I\}=\sum p_i$                    |
| $(X,Y)\sim f(x,y)$ | $\displaystyle{P\{(X,Y)\in I\}=\iint_{I\cap G} f(x,y)dxdy}$（$G$是有效取值范围）<br />$\displaystyle{P\{X\in D|Y=y_0\}=\int_D f_{X|Y}(x|y=y_0)dx}$<br />$P\{X\in D_1|Y\in D_2\}=\dfrac{P\{X \in D_1,Y\in D_2\}}{P\{Y\in D_2\}}$ |
| $(X,Y)$离散+混合   |                          全概率公式                          |

注意：

使用全概率公式时，可以直接用$P(B)=\sum P(A_iB)$

如$P(X=Y,Y=1)$，由等价事件得 $=P(X=1,Y=1)$，由独立得 $=P(X=1)P(Y=1)$

## 变量独立性

| X，Y相互独立                                                 |                                |
| ------------------------------------------------------------ | ------------------------------ |
| 混合型                                                       | $F(x,y)=F_X(x)\cdot F_Y(y)$    |
| 二维离散型                                                   | $p_{ij}=p_{i\cdot}p_{\cdot j}$ |
| 二维连续型                                                   | $f(x,y)=f_X(x)f_Y(y)$          |
| 若f（x，y）的非零区域不是矩形（包括无界广义矩形），则X与Y不独立 |                                |
| X,Y的函数g(X),g(Y)也相互独立                                 |                                |

# 常见分布

![image-20231027172146099](_media/pic//image-20231027172146099.png)

# 五.二维函数

## 多维$\to$一维

### (离散,离散)$\to$离散

把所有(X,Y)情况的概率都求出来，再和$g(x,y)$一一对应得到Z的分布列

### (连续,连续)$\to$连续

#### 分布函数法

核心：通过$f(x,y)$将$F_Z(z)$求解出来
$$
F_Z(z)=P\{Z\le z\}=P\{g(X,Y)\le z\}=\iint_{g(x,y)\le z}f(x,y)dxdy
$$

1.画出有效区间和$g(x,y)-z\le 0$的区域（将z视为参数，横轴为x，纵轴为y）

2.通过z的变化，分区间求定积分

#### 卷积公式法

>几乎万能

$$
f_Z(z)=\int |g'(z)|~f(g(z),y)~dy
\\
若X,Y独立:~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\\
f_Z(z)=\int |g'(z)|~f_X(g(z))~f_Y(y)~dy
$$

口诀：积谁不换谁，换完求z导的绝对值



1.将X用Z来表示得到$X=g(Z)$，求出$f(g(z), y)$的表达式

2.画出y为横轴，z为数轴的新区域

3.讨论z的取值范围确定y的取值范围

#### 最值函数

最大值：
$$
\begin{align}
F_{max}(z)&=P\{max\{X,Y\}\le z\}=P\{X\le z,Y\le z\}&&&&&&&&&&&&&&&&&&&&&&&
\\\\
若X,Y独立:
\\
F_{max}&=F_X(x)F_Y(x)
\\\\
若X,Y同分布:
\\
F_{max}&=F(z)^2
\end{align}
$$
最小值：
$$
\begin{align}
F_{min}(z)&=P\{min\{X,Y\}\le z\}&&&&&&&&&&&&&&&&&&&&&&
\\&=P\{X\le z\}\cup P\{Y\le z\}
\\&=P\{X\le z\}+P\{Y\le z\}-P\{X\le z,Y\le z\}
\\\\
若X,Y独立:
\\
F_{min}(z)&=1-P\{min\{X,Y\}> z\}
\\&=1-P\{X>z,Y>y\}
\\&=1-[1-P\{X\le z\}][1-P\{Y\le z\}]
\\&=1-[1-F_X(z)][1-F_Y(z)]
\\\\
若X,Y同分布:
\\
F_{min}&=1-[1-F(z)]^2
\\
f_{min}&=2[1-F(z)]f(z)
\end{align}
$$

### (离散,连续)$\to$连续

$X\sim p_i,Y\sim f_y(y),Z=g(X,Y)$

$F_Z(z)=P\{Z\le z\}=P\{g(X,Y)\le z\}$

|           | 本质区别：独立情况下条件概率可以直接去掉条件   |
| --------- | ---------------------------------------------- |
| X,Y独立   | 全概率公式（离散型入手）+ 一维函数的分布函数法 |
| X,Y不独立 | 等价事件（分情况讨论）+ 分布函数法             |

## 多维$\to$多维

<img src="_media/pic//image-20231113172633183.png" alt="image-20231113172633183" style="zoom:80%;" />

注意：共同特点是构造出来的多维随机变量一定是**（离散，离散）**

题目会让你求（离散，离散）的联合分布函数

# 六.数字特征

标准化随机变量：$E=0,D=1$

## 常用期望/方差

![image-20231101200816810](_media/pic//image-20231101200816810.png)

### 正态分布

一维正态分布



二维正态分布

$N(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho)$

$EX=\mu_1,EY=\mu_2,DX=\sigma_1^2,DY=\sigma_2^2,\rho_{X,Y}=\rho$

### 分布的可加性

> 指同一类型分布的**独立**随机变量之和的分布仍属于此类分布

|                        |                                                  |
| ---------------------- | ------------------------------------------------ |
| 二项分布（单项可加性） | $X+Y\sim B(m+n, p)$                              |
| 泊松分布               | $X+Y\sim P(\lambda_1+\lambda_2)$                 |
| 正态分布               | $X+Y\sim N(\mu_1+\mu_2, \sigma_1^2+ \sigma_2^2)$ |
| 卡方分布               | $X+Y\sim \chi^2_{m+n}$                           |



## 分解思想

求期望一般两种方法

1.直接写出分布律	2.分解法

对于n种情况难以列出全部的概率，故必须使用分解法：只研究一次试验的指示变量的分布情况，求出它的期望就能求出总期望



随机变量的分解为一次试验的指示变量$U_i$

$EX=E(\sum U_i)=\sum E(U_i)$

情况1：$E(U_i)$和事件发生的次数无关，为定值（比如抽彩票，配对问题）

情况2：$E(U_i)$和事件发生的次数有关 

## 期望

> 合理的加权平均

| 性质         |                                   |
| ------------ | --------------------------------- |
|              | $Ea=a~,~E(EX)=EX$（EX相当于常数） |
| 无条件打开   | $E(aX+bY)=aEX+bEY$                |
|              | 若X，Y独立，$E(XY)=EX\cdot EY$    |
| 求平方的期望 | $E(X^2)=DX+(EX)^2$                |

### 1. $X$

|              |                                                     |
| :----------: | :-------------------------------------------------: |
| $X\sim p_i$  |                  $EX=\sum x_ip_i$                   |
| $X\sim f(x)$ | $\displaystyle{EX=\int_{-\infin}^{+\infin}xf(x)dx}$ |

### 2. $g(X)$

|              |                                                        |
| :----------: | :----------------------------------------------------: |
| $X\sim p_i$  |                  $EY=\sum g(x_i)p_i$                   |
| $X\sim f(x)$ | $\displaystyle{EY=\int_{-\infin}^{+\infin}g(x)f(x)dx}$ |

### 3. $g(X,Y)$

$Z=g(X,Y)$

**当Z=X或Y时，可求得EX，EY**

|                    |                                                              |
| :----------------: | :----------------------------------------------------------: |
| $(X,Y)\sim p_{ij}$ |               $EZ=\sum\sum g(x_i, y_i)p_{ij}$                |
|                    |                $EX=\sum\sum x_i\cdot p_{ij}$                 |
| $(X,Y)\sim f(x,y)$ | $\displaystyle{EZ=\int_{-\infin}^{+\infin}\int_{-\infin}^{+\infin}g(x,y)f(x,y)dxdy}$ |
|                    | $\displaystyle{EX=\int_{-\infin}^{+\infin}\int_{-\infin}^{+\infin}xf(x,y)dxdy}$ |

### 4.最值

![image-20231113173746642](_media/pic//image-20231113173746642.png)

![image-20231214162515565](_media/pic//image-20231214162515565.png)

## 方差

$$
定义/核心公式:\\
DX=E[(X-EX)^2]=E(X^2)-(EX)^2
$$

| 性质                          |
| ----------------------------- |
| $DX\ge0$                      |
| $Dc=0$                        |
| $D(aX+b)=a^2DX$               |
| $D(X\pm Y)=DX+DY\pm2Cov(X,Y)$ |
| 若X，Y独立，$D(XY)\ge DXDY$   |

<img src="_media/pic//image-20231126171929593.png" alt="image-20231126171929593" style="zoom: 80%;" />



### 凑期望，方差

![image-20231109110941728](_media/pic//image-20231109110941728.png)



## 协方差

> 意义：判断两个变量在变化过程中是同方向变化（正相关）？还是反方向变化（负相关）？同向或反向程度如何？
>
> 刻画两个变量的相关程度

$$
Cov(X,Y)=E((X-EX)(Y-EY))=E(XY)-EXEY
$$

| 性质       |                                        |
| ---------- | -------------------------------------- |
| 对称性     | $Cov(X,Y)=Cov(Y,X)$                    |
|            | $Cov(X,X)=DX$                          |
| 单个可拆性 | $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$ |
| 线性性     | $Cov(aX+b,cY+d)=ac\cdot Cov(X,Y)$      |

​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     

## 相关系数

> 刻画两个变量之间**线性**相关性
>

$$
\rho_{XY}=\dfrac{Cov(X,Y)}{\sqrt{DX}\sqrt{DY}}
$$

![image-20231109114022439](_media/pic//image-20231109114022439.png)

| 性质                                                         |
| ------------------------------------------------------------ |
| $\|\rho\|\le1$                                                 |
| ![image-20231113195504383](_media/pic//image-20231113195504383.png) |

## 变量相关性

独立一定不相关，不相关不一定独立

特别的，当随机变量x,y是服从于二维正态分布时，不相关 = 独立

| 等价命题       |
| -------------- |
| $X,Y$不相关    |
| $E(XY)=EXEY$   |
| $D(X+Y)=DX+DY$ |
| $Cov(X,Y)=0$   |



# 七.大数定理&中心极限定理

## 切比雪夫不等式

> 为证明 依概率收敛（大数定理）而服务

背景：E反映了随机变量取值的平均程度，D反映了随机变量取值和E的平均偏离程度，那么$P\{|X-EX|\ge\varepsilon\}$应该与DX有一定关系，切比雪夫不等式就是描述这种关系的
$$
P\{|X-EX|\ge\varepsilon\}\le\dfrac{D(X)}{\varepsilon^2}
\\
P\{|X-EX|<\varepsilon\}>1-\dfrac{D(X)}{\varepsilon^2}
$$

切比雪夫不等式的重要意义在于：

当仅知$E,D$时，可以得到$P\{|X-EX|<\varepsilon\}$的下界——$1-\dfrac{D(X)}{\varepsilon^2}$

注意：下界就是一个不准确的值，所以如果求$P\{a<X<b\}$的下界可以放缩成$P\{|X-EX|<\varepsilon\}$

## 依概率收敛

随机变量序列$X_1,X_2,...,X_n$依概率收敛于$a$
$$
lim_{n\to\infin}P\{|X_n-a|<\varepsilon\}=1\\
X_n\xrightarrow{P}a
$$

## 大数定理

> 本质：*随机变量序列的算数平均值* 依概率收敛于期望E

### 切比雪夫大数定律

| 满足：                     |
| -------------------------- |
| 1.随机变量序列相互**独立** |
| 2.$DX_i\le C$（常数）      |

令$Y_n=\dfrac{1}{n}\sum_{i=1}^nX_i$
$$
Y_n\xrightarrow{P}E(Y_n)
$$

### 辛钦大数定律

| 满足：                     |
| -------------------------- |
| 1.随机变量序列相互**独立** |
| 2.**同分布**               |
| 3.期望存在——$EX_i=\mu$     |

$$
\dfrac{1}{n}\sum_{i=1}^nX_i\xrightarrow{P}\mu
$$

## 中心极限定理

> 该定理是大样本统计推断的基础
>
> 一句话概括：大量独立随机变量和的分布满足正态分布

| 满足：                           |
| -------------------------------- |
| 1.随机变量序列相互**独立同分布** |
| 2.$EX_i=\mu,DX_i=\sigma^2$       |

$$
令Y_n=\sum X_i~~~~~~~~~~~~~~~~~~~~~~~~~~
\\\\
lim_{n\to\infin}P\{\dfrac{Y_n-n\mu}{\sqrt{n}\sigma}\le x\}=\Phi(x)
\\\\
Y_n\sim N(n\mu,n\sigma^2)
$$

### 近似二项分布

| n重伯努利实验$\sum X_i=Y\sim B(n,p)$ |                          |                                                              |
| ------------------------------------ | ------------------------ | ------------------------------------------------------------ |
| n不太大时（n<10）                    | 二项分布                 | $P\{Y=k\}=C_n^k~p^k(1-p)^{n-k}$                              |
| n较大且p较小时，$\lambda=np$适中     | 泊松近似                 | $P\{Y=k\}=\dfrac{\lambda^ke^{-\lambda}}{k!}$                 |
| n较大且p不大时                       | 正态近似（中心极限定理） | $P\{a<Y\le b\}\approx\Phi(\dfrac{b-np}{\sqrt{np(1-p)}})-\Phi(\dfrac{a-np}{\sqrt{np(1-p)}})$ |



# 使用伽马函数简化运算

![image-20231126164531995](_media/pic//image-20231126164531995.png)



# 八.数理统计

>概率论是已知总体服从什么分布，从而推断出这个分布有什么样的性质，比如已知分布，求期望方差；
>
>数理统计好比总体是未知的，通过从总体中抽取的样本，**目的**是来推断总体具有什么样的特点。
>
>比如，数理统计中样本的统计量（样本方差，样本均值）已知，但是总体的分布是未知的。
>
>这个总体分布未知有时候是总体分布类型未知，有时候是总体分布类型已知但分布的参数未知。（如假定总体服从正态分布，但均值方差未知）
>
>一般概率论数理统计这本教材中数理统计内容主要围绕推断分布参数展开。
>
>数理统计分两块，参数估计和假设检验，而这两类都是关于总体分布的推断。
>
>参数估计：
>
>1.    点估计：为未知参数挑选出一个可能值
>2.    区间估计：为未知参数挑选出一个可能取值区间
>
>假设检验：检验未知参数是否为这个值或者在这个区间内。
>
>一言以蔽之，数理统计好比是概率论的“逆运算”。

|                  |                                                |
| ---------------- | ---------------------------------------------- |
| 总体             | 研究对象的某项数量指标的全体$X$                |
| （简单随机）样本 | **独立同分布**的n个个体组成$(X_1,X_2,...,X_n)$ |
| 样本观测值       | $(x_1,x_2,...,x_n)$                            |

## 统计量

不含未知参数的$g(X_1,X_2,...,X_n)$

### 样本矩

![image-20231115135111366](_media/pic//image-20231115135111366.png)

|                     |
| :-----------------: |
| $E\overline{X}=\mu$ |
|   $ES^2=\sigma^2$   |



### 次序统计量

![image-20231115135336643](_media/pic//image-20231115135336643.png)

|            | 概率密度函数              |
| ---------- | ------------------------- |
| 最小次序量 | $f_1=n[1-F(x)]^{n-1}f(x)$ |
| 最大次序量 | $f_n=n[F(x)]^{n-1}f(x)$   |

#### 求次序统计量的期望和方差

1.先求概率密度函数

## 三大分布

### $\chi^2$分布

前提：$X_1,X_2,...,X_n\sim N(0,1)$**相互独立**

构成：$Y=\sum^n_{i=1} X_i^2$

服从自由度（独立变量的个数）为n的$\chi^2$分布，$Y\sim \chi^2_n$

| 性质                                                         |
| ------------------------------------------------------------ |
| $EY=n,DY=2n$                                                 |
| $Y_1\sim\chi^2_m,Y_2\sim\chi^2_n$，且$Y_1,Y_2$相互独立，则$Y_1+Y_2\sim \chi^2_{m+n}$ |

### $t$分布

前提：$X\sim N(0,1),Y\sim \chi^2_n,X,Y$**相互独立**

构成：$T=\dfrac{X}{\sqrt{\dfrac{Y}{n}}}$

服从自由度为n的$t$分布，$T\sim t_n$

| 性质   |
| ------ |
| $ET=0$ |

### $F$分布

前提：$Y_1\sim \chi^2_{n_1},Y_2\sim \chi^2_{n_2},Y_1,Y_2$**相互独立**

构成：$F=\dfrac{\dfrac{Y_1}{n_1}}{\dfrac{Y_2}{n_2}}$

服从自由度为$(n_1,n_2)$的$F$分布，$F\sim F_{n_1,n_2}$

| 性质                                                |
| --------------------------------------------------- |
| $F\sim F(n_1,n_2)$，则$\dfrac{1}{F}\sim F(n_2,n_1)$ |

## 抽样分布

> 抽样分布：统计量的分布

### 单个正态总体

$(X_1,X_2,...,X_n)$是$X\sim N(\mu, \sigma^2)$的样本
$$
\begin{align}
&&&&&&&&&&&&&&&&
\\
1.\dfrac{\overline{X}-\mu}{\sigma/\sqrt{n}}&\sim N(0,1)
\\
2.\dfrac{\overline{X}-\mu}{S/\sqrt{n}}&\sim t_{n-1}
\\
3.\sum_{i=1}^n(\dfrac{X_i-\overline{X}}{\sigma})^2=\dfrac{(n-1)S^2}{\sigma^2}&\sim \chi^2_{n-1}

\end{align}
$$

### 两个正态总体

$$
\begin{align}
\dfrac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{S_w\sqrt{\dfrac{1}{m}+\dfrac{1}{n}}}&\sim t_{m+n-2}
\\\\
\dfrac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}&\sim F_{m-1,n-1}&&&&&&&&&&
\end{align}
$$

## 根据抽样分布求统计量的期望方差

$$
\begin{align}
E(\overline{X}):&&&&&&&&&&&&&&&&
\\
&E\overline{X}=\dfrac{1}{n}\sum EX_i=\mu
\\
D(\overline{X}):
\\
&D\overline{X}=\dfrac{1}{n^2}\sum DX_i=\dfrac{\sigma^2}{n}
\\
E(S^2) ~~\&~~D(S^2):
\\
&因为:\dfrac{(n-1)S^2}{\sigma^2}\sim \chi^2_{n-1}
\\
&故:\dfrac{n-1}{\sigma^2}E(S^2)=n-1,~~~~\dfrac{(n-1)^2}{\sigma^4}D(S^2)=2(n-1)
\\
&E(S^2)=\sigma^2
\\
&D(S^2)=\dfrac{2\sigma^4}{n-1}
\end{align}
$$

## 已知分布求参数

定一动一，将其中一个表达式化为标志形式，令另一个表达式满足N（0,1），通过D=1即可快速求出参数



# 参数估计

> 已知分布类型，参数未知，通过样本估计参数——参数估计

|        |                                              |
| ------ | -------------------------------------------- |
| 估计量 | $\hat{\theta}=\hat{\theta}(X_1,X_2,...,X_n)$ |
| 估计值 | $\hat{\theta}=\hat{\theta}(x_1,x_2,...,x_n)$ |

## 点估计

### 矩估计

> 原理：根据大数定律，用样本矩（样本均值，样本方差）替换总体矩（期望，方差）估计参数
>
> $\theta=EX$
>
> $\hat{\theta}=\overline{X}$
>
> 缺点：没用充分利用分布对参数的影响

实际解题中，只有1个or2个参数

|         |                                                              |
| ------- | ------------------------------------------------------------ |
| 1个参数 | $EX=\overline{X}$                                            |
| 2个参数 | $\begin{align}&EX=\overline{X}~~~(一阶原点矩)\\&DX+(EX)^2=\dfrac{1}{n}\sum X_i^2(二阶原点矩)\end{align}$ |

根据离散型的分布列/连续型的概率密度求：EX和DX 

结论：无论分布是什么，$\hat{\mu}=\overline{X},\hat{\sigma^2}=M_2$

### 最大似然估计

>选取 $\theta$ 使得样本观测值出现的概率最大，即求似然函数的最大值

| 1.写似然函数 |                                                              |
| ------------ | ------------------------------------------------------------ |
| 连续型       | $\displaystyle{L(\theta)=\prod_{i=1}^nf(x_i)}$               |
| 离散型       | $\displaystyle{L(\theta)=\prod_{i=1}^nP\{X=x_i\}}$<br />如果没有给出样本观测值，需要假设有$m$个$x_1$，$k$个$x_2$，$n-m-k$个$x_3$，相应的$P\{X=x_1\}$就是$m$次方... |
| 对数似然函数 | $l= lnL$                                                     |

| 2.求参数（似然函数三种情况） |                                                              |
| ---------------------------- | ------------------------------------------------------------ |
| 有驻点                       | 如果解不唯一，通过2阶导数正负取舍（要导数<0的）              |
| 单调                         | 用次序统计量（可能会有双参数的情况，一个取最大，一个取最小） |
| 常数                         | 用次序统计量，区间内任意一点都是最大似然估计值               |

常用$n\overline{X}$替换$\sum X_i$

### 估计量的评价

|                      |                                                              |                            |
| -------------------- | ------------------------------------------------------------ | -------------------------- |
| 无偏性               | $E\hat{\theta}=\theta$                                       | $\hat{\theta}$是无偏估计量 |
|                      | $E\overline{X}=\mu\\ES^2=\sigma^2$                           |                            |
|                      | $\hat{\theta}$是$\theta$的无偏估计量，一般仅当$f(x)$是线性函数时，$f(\hat{\theta})$也是$f(\theta)$的无偏估计量 |                            |
| 有效性               | $E\hat{\theta_1}=E\hat{\theta_2}=\theta,D\theta_1 <D\theta_2$ | 则$\theta_1$更有效         |
| 相合估计（一致估计） | $lim_{n\to \infin}P\{|\hat{\theta}_n-\theta|\ge\varepsilon\}=0$ | $\hat{\theta}$是相合估计量 |
|                      | 因为$lim_{n\to \infin}P\{|\hat{\theta}_n-\theta|\ge\varepsilon\}\le\dfrac{D(\hat{\theta})}{\varepsilon^2}$<br />证明$lim_{n\to\infin}D(\hat{\theta})=0$即可 |                            |

## 区间估计

>区间估计是找到一个随机的区间，使得区间包含真实参数的概率等于一个固定的值1-α，我们称这个值为置信水平

区间估计的长度越大，精确度（用区间估计的长度衡量）越低，置信度（用*区间包含未知参数*的概率衡量）越大

​        实际上我们总是保证置信度的条件下（已知确定的置信度），提高精确度
$$
P\{\hat{\theta_1}<\theta<\hat{\theta_2}\}= 1-\alpha
$$

|                    |                                         |
| ------------------ | --------------------------------------- |
| 置信度（置信水平） | $1-\alpha$                              |
| 置信区间           | $(\hat{\theta_1},\hat{\theta_2})$       |
| 边际误差           | $\hat{\theta_2}(\hat{\theta_1})-\theta$ |
| 枢轴量             | 只包含待估参数且分布已知的函数          |


枢轴量的选取原则

1.包含待估参数

2.包含已知参数

3.不包含其他未知参数

构造枢轴量的原理

一般通过参数$\theta$的点估计来构造：将点估计量如（$\overline{X}$）与待估参数（如$\mu$）复合在一起成为一个枢轴量。由于枢轴变量的表达式含有待估参数有关，因而不是统计量，这是一种常用构造枢轴量的方法

### 上侧分位数

$$
P\{X\ge x_{\alpha}\}=\alpha
\\
则x_{\alpha}是上侧分位数
$$

<img src="_media/pic//image-20231125200809065.png" alt="image-20231125200809065" style="zoom:67%;" />

为什么要对称取值：为了使置信区间最短，精确度最高

| 上侧分位数的性质 |                                                              |
| ---------------- | ------------------------------------------------------------ |
| 正态分布，t分布  | $z(1-\dfrac{\alpha}{2})=-z(\dfrac{\alpha}{2})$<br />$t_{n-1}(1-\dfrac{\alpha}{2})=-t_{n-1}(\dfrac{\alpha}{2})$ |
| $\chi^2$分布     | 无                                                           |
| F分布            | $F_{n,m}(1-\alpha)=\dfrac{1}{F_{m,n}(\alpha)}$               |

### 双侧置信区间

#### 一个正态总体参数

|              |                |                                                          |
| ------------ | -------------- | -------------------------------------------------------- |
| $\mu$：      | $\sigma^2$已知 | $Z=\dfrac{\sqrt n(\overline{X}-\mu)}{\sigma}\sim N(0,1)$ |
| $\mu$：      | $\sigma^2$未知 | $T=\dfrac{\sqrt n(\overline{X}-\mu)}{S}\sim t_{n-1}$     |
| $\sigma^2$： | $\mu$未知      | $\chi^2=\dfrac{(n-1)S^2}{\sigma^2}\sim \chi^2_{n-1}$     |

#### 两个正态总体参数

|                                    |                             |                                                              |
| ---------------------------------- | --------------------------- | ------------------------------------------------------------ |
| $\mu_1-\mu_2$：                    | $\sigma_1^2,\sigma_2^2$已知 | $Z=\dfrac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{\sqrt{\dfrac{\sigma_1^2}{m}+\dfrac{\sigma_2^2}{n}}}\sim N(0,1)$ |
| $\mu_1-\mu_2$：                    | $\sigma_1^2,\sigma_2^2$未知 | $T=\dfrac{(\overline{X}-\overline{Y})-(\mu_1-\mu_2)}{S_w\sqrt{\dfrac{1}{m}+\dfrac{1}{n}}}\sim t_{m+n-2}$ |
| $\dfrac{\sigma_1^2}{\sigma_2^2}$： |                             | $F=\dfrac{S_2^2/\sigma_2^2}{S_1^2/\sigma_1^2}=\dfrac{S_2^2*\sigma_1^2}{S_1^2*\sigma_2^2}\sim F_{n-1,m-1}$ |

### 单侧置信区间

> 有时候我们不关心上限，比如灯泡寿命，只要下限不低于某个值就行，就可以使用单侧置信区间，因为相比于双侧，它的边际误差更小，置信区间更精准

$$
P\{\hat{\theta}<\theta\}= 1-\alpha
$$

实际上，单侧置信区间的上下限只是将双侧置信区间的上下限中的$\dfrac{\alpha}{2}$换成了$\alpha$

# 假设检验

> 对总体的分布类型或分布类型中的未知参数提出假设
>
> 目的：判断某个假设是否真实，使得在【原假设】真实的情况下，犯错的概率等于一个固定的值（显著性水平$\alpha$）

|                                                |                                      |
| ---------------------------------------------- | ------------------------------------ |
| 原假设$H_0$                                    | 没有充分理由不能轻易否定的假设       |
| 备择假设$H_1$                                  | 原假设的否定，但不一定是对立事件     |
| 显著性水平$\alpha$                             | 某事件概率<$\alpha$，就是小概率事件  |
| 拒绝域$W=\{(X_1,X_2,...,X_n)|Z\in I(有等号)\}$ | 有利于$H_1$的区域                    |
| 检验统计量                                     | 对应置信区间的枢轴量，将待估参数换成 |

   <img src="_media/pic//image-20231128200214976.png" alt="image-20231128200214976" style="zoom: 67%;" />

为什么要对称取值，为了使接受域最小，犯第二类错误的概率最小

![image-20231128200836825](_media/pic//image-20231128200836825.png)

置信区间和接受域非常相似

| 假设检验的步骤                   |
| -------------------------------- |
| 1.提出原假设和备择假设           |
| 2.选取检验统计量                 |
| 3.计算拒绝域和检验统计量的观测值 |
| 4.决策                           |

## 假设的选取

1.“拒绝域”有“充分性”，而“接受域”没有“充分性”

2.“等号”一般在“原假设”里

| 规则                                             | 例子                             |
| ------------------------------------------------ | -------------------------------- |
| **成立但是误判为不成立后果严重**的命题作为原假设 | 检验药物是否有效                 |
| **隐含的常识或阐述的结论**的命题作为原假设       | 检验吸烟是否有害健康             |
| **想要验证的**（是否可以认为）命题当做备择假设   | 检验是否可以认为早晨身高高于晚上 |

**终极答案：**

从本质上说明如何选取备择假设：

我们进行了一次抽样实验，但得到了*与我们曾经认可的结论* 相悖的结果，于是我们想检验一下这个结果是否能推翻我们的原认知，所以将作为备择假设

如果没有原认知，就将我们想验证的那个命题作为备择假设，比如我想检验 *早晨身高高于晚上*，那就H_1：早晨>晚上，我想检验 *早晨身高低于晚上*，那就H_1：早晨<晚上

## 两类错误

假设检验的指导思想：

犯第一类错误的概率不超过$\alpha$，犯第二类错误的概率$\beta$尽可能小

> 对比置信区间：实际上我们总是保证置信度的条件下（已知确定的置信度），提高精确度

<img src="_media/pic//image-20231202220002488.png" alt="image-20231202220002488" style="zoom:80%;" />

## 求拒绝域

$$
P\{假设检验量\in D\}=\alpha,求D
$$

单侧拒绝域就将$\dfrac{\alpha}{2}$换成$\alpha$

### 一个正态总体参数

|              |                |                                                            |
| ------------ | -------------- | ---------------------------------------------------------- |
| $\mu$：      | $\sigma^2$已知 | $Z=\dfrac{\sqrt n(\overline{X}-\mu_0)}{\sigma}\sim N(0,1)$ |
| $\mu$：      | $\sigma^2$未知 | $T=\dfrac{\sqrt n(\overline{X}-\mu_0)}{S}\sim t_{n-1}$     |
| $\sigma^2$： | $\mu$未知      | $\chi^2=\dfrac{(n-1)S^2}{\sigma_0^2}\sim \chi^2_{n-1}$     |

### 两个正态总体参数

|                                    |                             | 检验统计量                                                   | 拒绝域                                                       |
| ---------------------------------- | --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| $\mu_1-\mu_2$：                    | $\sigma_1^2,\sigma_2^2$已知 | $Z=\dfrac{\overline{X}-\overline{Y}}{\sqrt{\dfrac{\sigma_1^2}{m}+\dfrac{\sigma_2^2}{n}}}\sim N(0,1)$ | $\|Z\|\ge z(\dfrac{\alpha}{2})$                                |
| $\mu_1-\mu_2$：                    | $\sigma_1^2=\sigma_2^2$未知 | $T=\dfrac{\overline{X}-\overline{Y}}{S_w\sqrt{\dfrac{1}{m}+\dfrac{1}{n}}}\sim t_{m+n-2}$ | $\|T\|\ge t_{m+n-2}(\dfrac{\alpha}{2})$                        |
| $\dfrac{\sigma_1^2}{\sigma_2^2}$： | $\mu_1,\mu_2$未知           | $F=\dfrac{S_1^2}{S_2^2}\sim F_{m-1,n-1}$                     | $F_{m-1,n-1}(1-\dfrac{\alpha}{2}) \le F\le F_{m-1,n-1}(\dfrac{\alpha}{2})$ |

# 假设检验&参数估计的联系

[(99+ 封私信 / 9 条消息) 如何用易懂的语言解释区间估计和假设检验的区别和联系？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/26415058)

若对参数一无所知，用参数估计的方法处理

若对参数有所了解，但又怀疑猜测需要证实时，用假设检验的方法处理

